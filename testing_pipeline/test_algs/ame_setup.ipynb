{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IStGWeZl2U_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a03ce5fd-341e-4853-9ffb-2f1e28d7e39d"
      },
      "source": [
        "!git clone https://github.com/d909b/ame"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ame'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 35 (delta 0), reused 0 (delta 0), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (35/35), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30cn611BrCwS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxljHtpCrDFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('/content/ame')\n",
        "sys.path.append('/content/ame/ame_starter')\n",
        "sys.path.append('/content/ame/ame_starter/models')\n",
        "sys.path.append('/content/ame/ame_starter/apps')\n",
        "sys.path.append('/content/ame/ame_starter/data_access')\n",
        "os.environ['PYTHONPATH'] += \":/content/ame\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRujYyAIwf6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b746db9-f78c-4af5-acbe-4f49a8c16305"
      },
      "source": [
        "!echo $PYTHONPATH\n",
        "!echo $PATH\n",
        "!pwd\n",
        "#!python /content/ame/setup.py install"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/env/python:/content/ame\n",
            "/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin\n",
            "/content\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing ame_starter.egg-info/PKG-INFO\n",
            "writing dependency_links to ame_starter.egg-info/dependency_links.txt\n",
            "writing requirements to ame_starter.egg-info/requires.txt\n",
            "writing top-level names to ame_starter.egg-info/top_level.txt\n",
            "reading manifest file 'ame_starter.egg-info/SOURCES.txt'\n",
            "writing manifest file 'ame_starter.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ame_starter.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ame_starter.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ame_starter.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ame_starter.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ame_starter.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/ame_starter-1.0.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing ame_starter-1.0.0-py3.6.egg\n",
            "Removing /usr/local/lib/python3.6/dist-packages/ame_starter-1.0.0-py3.6.egg\n",
            "Copying ame_starter-1.0.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "ame-starter 1.0.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/ame_starter-1.0.0-py3.6.egg\n",
            "Processing dependencies for ame-starter==1.0.0\n",
            "Searching for numpy==1.18.5\n",
            "Best match: numpy 1.18.5\n",
            "Adding numpy 1.18.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scikit-learn==0.19.0\n",
            "Best match: scikit-learn 0.19.0\n",
            "Processing scikit_learn-0.19.0-py3.6-linux-x86_64.egg\n",
            "scikit-learn 0.19.0 is already the active version in easy-install.pth\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages/scikit_learn-0.19.0-py3.6-linux-x86_64.egg\n",
            "Searching for h5py==2.10.0\n",
            "Best match: h5py 2.10.0\n",
            "Adding h5py 2.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pandas==1.0.5\n",
            "Best match: pandas 1.0.5\n",
            "Adding pandas 1.0.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for matplotlib==3.2.2\n",
            "Best match: matplotlib 3.2.2\n",
            "Adding matplotlib 3.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras==2.2.0\n",
            "Best match: Keras 2.2.0\n",
            "Processing Keras-2.2.0-py3.6.egg\n",
            "Removing Keras 2.1.5 from easy-install.pth file\n",
            "Adding Keras 2.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages/Keras-2.2.0-py3.6.egg\n",
            "Searching for tensorflow==1.5.0\n",
            "Best match: tensorflow 1.5.0\n",
            "Processing tensorflow-1.5.0-py3.6-linux-x86_64.egg\n",
            "tensorflow 1.5.0 is already the active version in easy-install.pth\n",
            "Installing freeze_graph script to /usr/local/bin\n",
            "Installing saved_model_cli script to /usr/local/bin\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "Installing toco script to /usr/local/bin\n",
            "Installing toco_from_protos script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages/tensorflow-1.5.0-py3.6-linux-x86_64.egg\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.8.1\n",
            "Best match: python-dateutil 2.8.1\n",
            "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cycler==0.10.0\n",
            "Best match: cycler 0.10.0\n",
            "Adding cycler 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for kiwisolver==1.2.0\n",
            "Best match: kiwisolver 1.2.0\n",
            "Adding kiwisolver 1.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyparsing==2.4.7\n",
            "Best match: pyparsing 2.4.7\n",
            "Adding pyparsing 2.4.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for PyYAML==3.13\n",
            "Best match: PyYAML 3.13\n",
            "Adding PyYAML 3.13 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Preprocessing==1.0.1\n",
            "Best match: Keras-Preprocessing 1.0.1\n",
            "Processing Keras_Preprocessing-1.0.1-py3.6.egg\n",
            "Keras-Preprocessing 1.0.1 is already the active version in easy-install.pth\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages/Keras_Preprocessing-1.0.1-py3.6.egg\n",
            "Searching for Keras-Applications==1.0.2\n",
            "Best match: Keras-Applications 1.0.2\n",
            "Processing Keras_Applications-1.0.2-py3.6.egg\n",
            "Keras-Applications 1.0.2 is already the active version in easy-install.pth\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages/Keras_Applications-1.0.2-py3.6.egg\n",
            "Searching for wheel==0.34.2\n",
            "Best match: wheel 0.34.2\n",
            "Adding wheel 0.34.2 to easy-install.pth file\n",
            "Installing wheel script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorflow-tensorboard==1.5.1\n",
            "Best match: tensorflow-tensorboard 1.5.1\n",
            "Processing tensorflow_tensorboard-1.5.1-py3.6.egg\n",
            "tensorflow-tensorboard 1.5.1 is already the active version in easy-install.pth\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages/tensorflow_tensorboard-1.5.1-py3.6.egg\n",
            "Searching for protobuf==3.10.0\n",
            "Best match: protobuf 3.10.0\n",
            "Adding protobuf 3.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for absl-py==0.9.0\n",
            "Best match: absl-py 0.9.0\n",
            "Adding absl-py 0.9.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Werkzeug==1.0.1\n",
            "Best match: Werkzeug 1.0.1\n",
            "Adding Werkzeug 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Markdown==3.2.2\n",
            "Best match: Markdown 3.2.2\n",
            "Adding Markdown 3.2.2 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for html5lib==0.9999999\n",
            "Best match: html5lib 0.9999999\n",
            "Adding html5lib 0.9999999 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for bleach==1.5.0\n",
            "Best match: bleach 1.5.0\n",
            "Adding bleach 1.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for setuptools==47.3.1\n",
            "Best match: setuptools 47.3.1\n",
            "Adding setuptools 47.3.1 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "Installing easy_install-3.8 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for importlib-metadata==1.6.1\n",
            "Best match: importlib-metadata 1.6.1\n",
            "Adding importlib-metadata 1.6.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for zipp==3.1.0\n",
            "Best match: zipp 3.1.0\n",
            "Adding zipp 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for ame-starter==1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhPrv2FkuvFz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d40ccff9-bf54-4b54-bf8c-671b8228c9a1"
      },
      "source": [
        "!pip install tensorflow==1.4.0\n",
        "!pip install keras==2.1.5\n",
        "!pip install numpy==1.14.1\n",
        "#!pip install tensorflow==1.5.0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import keras as ks\n",
        "print(ks.__version__)\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "!mkdir output"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9f/be0165c6eefd841e6928e54d3d083fa174f92d640fdc52f73a33dc9c54d1/tensorflow-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (41.2MB)\n",
            "\u001b[K     |████████████████████████████████| 41.2MB 101kB/s \n",
            "\u001b[?25hCollecting enum34>=1.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/63/f6/ccb1c83687756aeabbf3ca0f213508fcfb03883ff200d201b3a4c60cedcc/enum34-1.1.10-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.4.0) (1.12.0)\n",
            "Collecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/9f/5845c18f9df5e7ea638ecf3a272238f0e7671e454faa396b5188c6e6fc0a/tensorflow_tensorboard-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.3.0->tensorflow==1.4.0) (47.3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (3.2.2)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0) (3.1.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=22fd014bffb21cd9059cebbfdb6ce36ae065efe2538ab1245c77c04d1ec58400\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: enum34, html5lib, bleach, tensorflow-tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.5\n",
            "    Uninstalling bleach-3.1.5:\n",
            "      Successfully uninstalled bleach-3.1.5\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed bleach-1.5.0 enum34-1.1.10 html5lib-0.9999999 tensorflow-1.4.0 tensorflow-tensorboard-0.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/65/e4aff762b8696ec0626a6654b1e73b396fcc8b7cc6b98d78a1bc53b85b48/Keras-2.1.5-py2.py3-none-any.whl (334kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.5) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.5) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.5) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.5) (1.4.1)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "Successfully installed keras-2.1.5\n",
            "Collecting numpy==1.14.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/7d/348c5d8d44443656e76285aa97b828b6dbd9c10e5b9c0f7f98eff0ff70e4/numpy-1.14.1-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2MB 346kB/s \n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.4 has requirement numpy>=1.17, but you'll have numpy 1.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2020.6.3 has requirement numpy>=1.15.1, but you'll have numpy 1.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.0.1.post1 has requirement numpy>=1.16, but you'll have numpy 1.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "Successfully installed numpy-1.14.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n",
            "2.1.5\n",
            "1.18.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81V6mfyOrMcA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c2de564-fb78-4a85-834b-59caf2a12a17"
      },
      "source": [
        "!python /content/ame/ame_starter/apps/main.py --dataset=\"boston_housing\" --batch_size=32 --num_epochs=300 --learning_rate=0.001 --output_directory='/content/drive/My Drive/pr2/ame_output' --do_train --do_evaluate --num_units=16 --num_layers=1 --early_stopping_patience=32"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
            "  return f(*args, **kwds)\n",
            "INFO: Args are: {'dataset': 'boston_housing', 'seed': 909, 'output_directory': '/content/drive/My Drive/pr2/ame_output', 'model_name': 'forecast.h5.npz', 'load_existing': '', 'n_jobs': 4, 'learning_rate': 0.001, 'l2_weight': 0.0, 'num_epochs': 300, 'batch_size': 32, 'early_stopping_patience': 32, 'num_units': 16, 'num_layers': 1, 'dropout': 0.0, 'granger_loss_weight': 0.03, 'fraction_of_data_set': 1, 'validation_set_fraction': 0.27, 'test_set_fraction': 0.1, 'num_hyperopt_runs': 35, 'hyperopt_offset': 0, 'do_train': True, 'do_hyperopt': False, 'do_evaluate': True, 'hyperopt_against_eval_set': False, 'copy_to_local': False, 'do_hyperopt_on_lsf': False, 'do_merge_lsf': False, 'with_tensorboard': False, 'save_predictions': True, 'save_attributions': True}\n",
            "INFO: Running at 2020-07-10 06:29:07.438921\n",
            "INFO: Seed is 909\n",
            "2020-07-10 06:29:07.439516: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "INFO: Run with args: {'dataset': 'boston_housing', 'seed': 909, 'output_directory': '/content/drive/My Drive/pr2/ame_output', 'model_name': 'forecast.h5.npz', 'load_existing': '', 'n_jobs': 4, 'learning_rate': 0.001, 'l2_weight': 0.0, 'num_epochs': 300, 'batch_size': 32, 'early_stopping_patience': 32, 'num_units': 16, 'num_layers': 1, 'dropout': 0.0, 'granger_loss_weight': 0.03, 'fraction_of_data_set': 1, 'validation_set_fraction': 0.27, 'test_set_fraction': 0.1, 'num_hyperopt_runs': 35, 'hyperopt_offset': 0, 'do_train': True, 'do_hyperopt': False, 'do_evaluate': True, 'hyperopt_against_eval_set': False, 'copy_to_local': False, 'do_hyperopt_on_lsf': False, 'do_merge_lsf': False, 'with_tensorboard': False, 'save_predictions': True, 'save_attributions': True}\n",
            "INFO: Loaded generator with 404 samples. Doing 13 steps of size 32\n",
            "INFO: Loaded generator with 102 samples. Doing 4 steps of size 32\n",
            "INFO: Loaded generator with 102 samples. Doing 4 steps of size 32\n",
            "INFO: Built generators with 13 training samples,  4 validation samples and 4 test samples.\n",
            "INFO: Started training feature extraction.\n",
            "Epoch 1/300\n",
            " - 1s - loss: 16532.1040 - combined_loss: 592.5968 - granger_loss: 0.3965 - repeat_loss: 15957.2731 - val_loss: 17112.2958 - val_combined_loss: 615.2784 - val_granger_loss: 0.6209 - val_repeat_loss: 16515.4573\n",
            "Epoch 2/300\n",
            " - 0s - loss: 16183.8810 - combined_loss: 583.4491 - granger_loss: 0.4920 - repeat_loss: 15617.9206 - val_loss: 16946.4490 - val_combined_loss: 613.4387 - val_granger_loss: 0.6542 - val_repeat_loss: 16351.3940\n",
            "Epoch 3/300\n",
            " - 0s - loss: 16009.1365 - combined_loss: 581.5461 - granger_loss: 0.5238 - repeat_loss: 15445.0211 - val_loss: 16714.7593 - val_combined_loss: 611.2113 - val_granger_loss: 0.4484 - val_repeat_loss: 16121.8711\n",
            "Epoch 4/300\n",
            " - 0s - loss: 15762.5934 - combined_loss: 579.4759 - granger_loss: 0.3726 - repeat_loss: 15200.4908 - val_loss: 16410.4273 - val_combined_loss: 608.4790 - val_granger_loss: 0.3288 - val_repeat_loss: 15820.1927\n",
            "Epoch 5/300\n",
            " - 0s - loss: 15430.0444 - combined_loss: 575.6254 - granger_loss: 0.2658 - repeat_loss: 14871.6796 - val_loss: 16001.4316 - val_combined_loss: 604.8865 - val_granger_loss: 0.2611 - val_repeat_loss: 15414.6838\n",
            "Epoch 6/300\n",
            " - 0s - loss: 15015.0290 - combined_loss: 572.0243 - granger_loss: 0.2423 - repeat_loss: 14460.1581 - val_loss: 15500.4280 - val_combined_loss: 600.3344 - val_granger_loss: 0.2314 - val_repeat_loss: 14918.0963\n",
            "Epoch 7/300\n",
            " - 0s - loss: 14677.7865 - combined_loss: 573.8184 - granger_loss: 0.2328 - repeat_loss: 14121.1757 - val_loss: 14915.6871 - val_combined_loss: 594.5492 - val_granger_loss: 0.2408 - val_repeat_loss: 14338.9672\n",
            "Epoch 8/300\n",
            " - 0s - loss: 13971.0382 - combined_loss: 563.1383 - granger_loss: 0.2637 - repeat_loss: 13424.7863 - val_loss: 14241.9192 - val_combined_loss: 586.9624 - val_granger_loss: 0.3103 - val_repeat_loss: 13672.5563\n",
            "Epoch 9/300\n",
            " - 0s - loss: 13156.2301 - combined_loss: 548.0675 - granger_loss: 0.3730 - repeat_loss: 12624.5934 - val_loss: 13508.7434 - val_combined_loss: 577.2793 - val_granger_loss: 0.4715 - val_repeat_loss: 12948.7681\n",
            "Epoch 10/300\n",
            " - 0s - loss: 12640.3791 - combined_loss: 544.3983 - granger_loss: 0.5961 - repeat_loss: 12112.2949 - val_loss: 12753.5277 - val_combined_loss: 566.0467 - val_granger_loss: 0.7563 - val_repeat_loss: 12204.4396\n",
            "Epoch 11/300\n",
            " - 0s - loss: 11871.5674 - combined_loss: 531.5633 - granger_loss: 0.9218 - repeat_loss: 11355.9232 - val_loss: 12004.7905 - val_combined_loss: 554.5999 - val_granger_loss: 1.0683 - val_repeat_loss: 11466.7966\n",
            "Epoch 12/300\n",
            " - 0s - loss: 11186.7121 - combined_loss: 521.4694 - granger_loss: 1.2216 - repeat_loss: 10680.8502 - val_loss: 11256.0421 - val_combined_loss: 542.9996 - val_granger_loss: 1.3232 - val_repeat_loss: 10729.2929\n",
            "Epoch 13/300\n",
            " - 0s - loss: 10515.9420 - combined_loss: 509.9486 - granger_loss: 1.4435 - repeat_loss: 10021.2485 - val_loss: 10554.0314 - val_combined_loss: 531.6608 - val_granger_loss: 1.5009 - val_repeat_loss: 10038.2756\n",
            "Epoch 14/300\n",
            " - 0s - loss: 9833.8994 - combined_loss: 497.2018 - granger_loss: 1.6079 - repeat_loss: 9351.5653 - val_loss: 9911.5098 - val_combined_loss: 520.5027 - val_granger_loss: 1.6309 - val_repeat_loss: 9406.5732\n",
            "Epoch 15/300\n",
            " - 0s - loss: 9300.5794 - combined_loss: 489.0174 - granger_loss: 1.7524 - repeat_loss: 8826.1800 - val_loss: 9336.3004 - val_combined_loss: 509.1737 - val_granger_loss: 1.7420 - val_repeat_loss: 8842.3496\n",
            "Epoch 16/300\n",
            " - 0s - loss: 8792.6430 - combined_loss: 476.2870 - granger_loss: 1.8560 - repeat_loss: 8330.5890 - val_loss: 8836.1498 - val_combined_loss: 497.7384 - val_granger_loss: 1.8623 - val_repeat_loss: 8353.2877\n",
            "Epoch 17/300\n",
            " - 0s - loss: 8363.3813 - combined_loss: 467.0928 - granger_loss: 1.9519 - repeat_loss: 7910.2428 - val_loss: 8420.2199 - val_combined_loss: 486.4717 - val_granger_loss: 1.9809 - val_repeat_loss: 7948.2830\n",
            "Epoch 18/300\n",
            " - 0s - loss: 7992.4164 - combined_loss: 455.1150 - granger_loss: 2.1138 - repeat_loss: 7550.8915 - val_loss: 8074.7169 - val_combined_loss: 475.1002 - val_granger_loss: 2.2131 - val_repeat_loss: 7613.8032\n",
            "Epoch 19/300\n",
            " - 0s - loss: 7596.8453 - combined_loss: 437.8635 - granger_loss: 2.4043 - repeat_loss: 7172.0456 - val_loss: 7786.9949 - val_combined_loss: 463.8621 - val_granger_loss: 2.5102 - val_repeat_loss: 7336.9734\n",
            "Epoch 20/300\n",
            " - 0s - loss: 7453.4168 - combined_loss: 432.8059 - granger_loss: 2.6115 - repeat_loss: 7033.5167 - val_loss: 7555.2024 - val_combined_loss: 453.4599 - val_granger_loss: 2.7759 - val_repeat_loss: 7115.2629\n",
            "Epoch 21/300\n",
            " - 0s - loss: 7185.4379 - combined_loss: 419.8297 - granger_loss: 2.9996 - repeat_loss: 6778.1130 - val_loss: 7364.1414 - val_combined_loss: 443.5016 - val_granger_loss: 3.0746 - val_repeat_loss: 6933.8526\n",
            "Epoch 22/300\n",
            " - 0s - loss: 7054.0086 - combined_loss: 411.6398 - granger_loss: 3.3556 - repeat_loss: 6654.6172 - val_loss: 7202.7688 - val_combined_loss: 433.8129 - val_granger_loss: 3.5600 - val_repeat_loss: 6781.8632\n",
            "Epoch 23/300\n",
            " - 0s - loss: 6885.6447 - combined_loss: 401.0441 - granger_loss: 3.9019 - repeat_loss: 6496.5148 - val_loss: 7061.5947 - val_combined_loss: 424.4084 - val_granger_loss: 3.9676 - val_repeat_loss: 6649.7995\n",
            "Epoch 24/300\n",
            " - 0s - loss: 6754.3480 - combined_loss: 391.2739 - granger_loss: 4.3166 - repeat_loss: 6374.6829 - val_loss: 6933.1410 - val_combined_loss: 415.0683 - val_granger_loss: 4.3518 - val_repeat_loss: 6530.3944\n",
            "Epoch 25/300\n",
            " - 0s - loss: 6616.5635 - combined_loss: 382.0473 - granger_loss: 4.5551 - repeat_loss: 6245.8411 - val_loss: 6816.9765 - val_combined_loss: 405.9372 - val_granger_loss: 4.6752 - val_repeat_loss: 6423.0773\n",
            "Epoch 26/300\n",
            " - 0s - loss: 6656.5652 - combined_loss: 381.5595 - granger_loss: 4.6755 - repeat_loss: 6286.3122 - val_loss: 6710.2003 - val_combined_loss: 397.1253 - val_granger_loss: 4.7726 - val_repeat_loss: 6324.8455\n",
            "Epoch 27/300\n",
            " - 0s - loss: 6402.0939 - combined_loss: 364.1391 - granger_loss: 4.7233 - repeat_loss: 6048.7372 - val_loss: 6601.1335 - val_combined_loss: 388.0497 - val_granger_loss: 4.8207 - val_repeat_loss: 6224.5806\n",
            "Epoch 28/300\n",
            " - 0s - loss: 6268.9019 - combined_loss: 353.6412 - granger_loss: 4.7098 - repeat_loss: 5925.7286 - val_loss: 6495.5061 - val_combined_loss: 379.0054 - val_granger_loss: 4.8364 - val_repeat_loss: 6127.7259\n",
            "Epoch 29/300\n",
            " - 0s - loss: 6207.8241 - combined_loss: 347.9145 - granger_loss: 4.7103 - repeat_loss: 5870.2058 - val_loss: 6393.8445 - val_combined_loss: 370.2140 - val_granger_loss: 4.8908 - val_repeat_loss: 6034.5901\n",
            "Epoch 30/300\n",
            " - 0s - loss: 6143.6406 - combined_loss: 341.2463 - granger_loss: 4.7146 - repeat_loss: 5812.4902 - val_loss: 6292.7140 - val_combined_loss: 361.3673 - val_granger_loss: 4.9860 - val_repeat_loss: 5942.0381\n",
            "Epoch 31/300\n",
            " - 0s - loss: 6013.4934 - combined_loss: 330.6856 - granger_loss: 4.7976 - repeat_loss: 5692.5845 - val_loss: 6189.3136 - val_combined_loss: 352.3193 - val_granger_loss: 5.0539 - val_repeat_loss: 5847.4123\n",
            "Epoch 32/300\n",
            " - 0s - loss: 5913.8417 - combined_loss: 321.8123 - granger_loss: 4.8603 - repeat_loss: 5601.5379 - val_loss: 6086.9496 - val_combined_loss: 343.3968 - val_granger_loss: 5.1027 - val_repeat_loss: 5753.7018\n",
            "Epoch 33/300\n",
            " - 0s - loss: 5831.6480 - combined_loss: 313.3753 - granger_loss: 4.9302 - repeat_loss: 5527.5261 - val_loss: 5987.6218 - val_combined_loss: 334.7504 - val_granger_loss: 5.1944 - val_repeat_loss: 5662.7580\n",
            "Epoch 34/300\n",
            " - 0s - loss: 5743.7481 - combined_loss: 306.3085 - granger_loss: 5.0189 - repeat_loss: 5446.4783 - val_loss: 5886.0651 - val_combined_loss: 325.8542 - val_granger_loss: 5.2708 - val_repeat_loss: 5569.8286\n",
            "Epoch 35/300\n",
            " - 0s - loss: 5626.1108 - combined_loss: 296.3725 - granger_loss: 5.0664 - repeat_loss: 5338.4775 - val_loss: 5784.3199 - val_combined_loss: 317.0593 - val_granger_loss: 5.3483 - val_repeat_loss: 5476.6118\n",
            "Epoch 36/300\n",
            " - 0s - loss: 5528.3180 - combined_loss: 287.0428 - granger_loss: 5.1437 - repeat_loss: 5249.7321 - val_loss: 5681.3158 - val_combined_loss: 308.1728 - val_granger_loss: 5.4149 - val_repeat_loss: 5382.2259\n",
            "Epoch 37/300\n",
            " - 0s - loss: 5491.8187 - combined_loss: 282.5214 - granger_loss: 5.2228 - repeat_loss: 5217.6162 - val_loss: 5578.4365 - val_combined_loss: 299.3796 - val_granger_loss: 5.4679 - val_repeat_loss: 5287.8743\n",
            "Epoch 38/300\n",
            " - 0s - loss: 5342.9191 - combined_loss: 271.9911 - granger_loss: 5.2903 - repeat_loss: 5078.9290 - val_loss: 5477.5393 - val_combined_loss: 290.8014 - val_granger_loss: 5.5241 - val_repeat_loss: 5195.2961\n",
            "Epoch 39/300\n",
            " - 0s - loss: 5211.8515 - combined_loss: 261.1833 - granger_loss: 5.3754 - repeat_loss: 4958.3425 - val_loss: 5374.6951 - val_combined_loss: 282.1515 - val_granger_loss: 5.5922 - val_repeat_loss: 5100.8405\n",
            "Epoch 40/300\n",
            " - 0s - loss: 5123.6542 - combined_loss: 254.2368 - granger_loss: 5.4406 - repeat_loss: 4876.8813 - val_loss: 5271.9535 - val_combined_loss: 273.6658 - val_granger_loss: 5.6533 - val_repeat_loss: 5006.3280\n",
            "Epoch 41/300\n",
            " - 0s - loss: 5096.1863 - combined_loss: 250.2233 - granger_loss: 5.5079 - repeat_loss: 4853.3045 - val_loss: 5171.0888 - val_combined_loss: 265.3449 - val_granger_loss: 5.7205 - val_repeat_loss: 4913.5325\n",
            "Epoch 42/300\n",
            " - 0s - loss: 4940.1282 - combined_loss: 238.9315 - granger_loss: 5.6000 - repeat_loss: 4708.1967 - val_loss: 5064.9402 - val_combined_loss: 256.8304 - val_granger_loss: 5.7819 - val_repeat_loss: 4815.6412\n",
            "Epoch 43/300\n",
            " - 0s - loss: 4913.5140 - combined_loss: 234.6566 - granger_loss: 5.6484 - repeat_loss: 4685.7277 - val_loss: 4961.6872 - val_combined_loss: 248.6835 - val_granger_loss: 5.8188 - val_repeat_loss: 4720.2897\n",
            "Epoch 44/300\n",
            " - 0s - loss: 4716.0194 - combined_loss: 221.5494 - granger_loss: 5.6924 - repeat_loss: 4500.9456 - val_loss: 4858.5685 - val_combined_loss: 240.7718 - val_granger_loss: 5.8051 - val_repeat_loss: 4624.8457\n",
            "Epoch 45/300\n",
            " - 0s - loss: 4710.7364 - combined_loss: 220.2944 - granger_loss: 5.7895 - repeat_loss: 4496.8772 - val_loss: 4757.1669 - val_combined_loss: 233.0387 - val_granger_loss: 5.8720 - val_repeat_loss: 4530.9433\n",
            "Epoch 46/300\n",
            " - 0s - loss: 4575.8200 - combined_loss: 210.5278 - granger_loss: 5.8494 - repeat_loss: 4371.4325 - val_loss: 4651.1837 - val_combined_loss: 225.1677 - val_granger_loss: 5.9416 - val_repeat_loss: 4432.5929\n",
            "Epoch 47/300\n",
            " - 0s - loss: 4430.1700 - combined_loss: 200.5551 - granger_loss: 5.9088 - repeat_loss: 4235.4542 - val_loss: 4546.5631 - val_combined_loss: 217.6391 - val_granger_loss: 5.9930 - val_repeat_loss: 4335.2734\n",
            "Epoch 48/300\n",
            " - 0s - loss: 4370.1953 - combined_loss: 196.0325 - granger_loss: 5.9705 - repeat_loss: 4179.8648 - val_loss: 4443.5024 - val_combined_loss: 210.2693 - val_granger_loss: 6.0397 - val_repeat_loss: 4239.3601\n",
            "Epoch 49/300\n",
            " - 0s - loss: 4289.0173 - combined_loss: 190.0939 - granger_loss: 6.0243 - repeat_loss: 4104.4455 - val_loss: 4340.5302 - val_combined_loss: 203.0690 - val_granger_loss: 6.0843 - val_repeat_loss: 4143.3708\n",
            "Epoch 50/300\n",
            " - 0s - loss: 4130.8024 - combined_loss: 179.6964 - granger_loss: 6.0443 - repeat_loss: 3956.3156 - val_loss: 4237.1133 - val_combined_loss: 196.0491 - val_granger_loss: 6.1103 - val_repeat_loss: 4046.7623\n",
            "Epoch 51/300\n",
            " - 0s - loss: 4056.1178 - combined_loss: 175.1102 - granger_loss: 6.0663 - repeat_loss: 3886.0790 - val_loss: 4137.6758 - val_combined_loss: 189.4654 - val_granger_loss: 6.1231 - val_repeat_loss: 3953.7106\n",
            "Epoch 52/300\n",
            " - 0s - loss: 4006.2707 - combined_loss: 172.1603 - granger_loss: 6.0919 - repeat_loss: 3839.0925 - val_loss: 4038.8531 - val_combined_loss: 183.0663 - val_granger_loss: 6.1790 - val_repeat_loss: 3861.0934\n",
            "Epoch 53/300\n",
            " - 0s - loss: 3904.7212 - combined_loss: 164.9987 - granger_loss: 6.0742 - repeat_loss: 3744.4902 - val_loss: 3938.1172 - val_combined_loss: 176.6851 - val_granger_loss: 6.1970 - val_repeat_loss: 3766.5467\n",
            "Epoch 54/300\n",
            " - 0s - loss: 3791.6258 - combined_loss: 158.3450 - granger_loss: 6.1301 - repeat_loss: 3637.8474 - val_loss: 3837.2948 - val_combined_loss: 170.4692 - val_granger_loss: 6.2339 - val_repeat_loss: 3671.7527\n",
            "Epoch 55/300\n",
            " - 0s - loss: 3725.7880 - combined_loss: 154.4496 - granger_loss: 6.1657 - repeat_loss: 3575.7869 - val_loss: 3738.6085 - val_combined_loss: 164.6730 - val_granger_loss: 6.2585 - val_repeat_loss: 3578.6879\n",
            "Epoch 56/300\n",
            " - 0s - loss: 3596.7708 - combined_loss: 147.5670 - granger_loss: 6.1587 - repeat_loss: 3453.4460 - val_loss: 3643.8142 - val_combined_loss: 159.1941 - val_granger_loss: 6.2679 - val_repeat_loss: 3489.2078\n",
            "Epoch 57/300\n",
            " - 0s - loss: 3494.1989 - combined_loss: 141.4115 - granger_loss: 6.1640 - repeat_loss: 3356.8449 - val_loss: 3549.4435 - val_combined_loss: 153.8345 - val_granger_loss: 6.2825 - val_repeat_loss: 3400.0355\n",
            "Epoch 58/300\n",
            " - 0s - loss: 3406.8281 - combined_loss: 137.0868 - granger_loss: 6.2050 - repeat_loss: 3273.6677 - val_loss: 3454.0511 - val_combined_loss: 148.5436 - val_granger_loss: 6.2706 - val_repeat_loss: 3309.7757\n",
            "Epoch 59/300\n",
            " - 0s - loss: 3381.6235 - combined_loss: 135.6183 - granger_loss: 6.1729 - repeat_loss: 3249.8886 - val_loss: 3364.3697 - val_combined_loss: 143.7447 - val_granger_loss: 6.2897 - val_repeat_loss: 3224.7487\n",
            "Epoch 60/300\n",
            " - 0s - loss: 3283.9864 - combined_loss: 130.1092 - granger_loss: 6.1674 - repeat_loss: 3157.5954 - val_loss: 3272.6935 - val_combined_loss: 138.9883 - val_granger_loss: 6.2734 - val_repeat_loss: 3137.6868\n",
            "Epoch 61/300\n",
            " - 0s - loss: 3137.4947 - combined_loss: 123.5157 - granger_loss: 6.1997 - repeat_loss: 3017.4985 - val_loss: 3184.0668 - val_combined_loss: 134.6360 - val_granger_loss: 6.2176 - val_repeat_loss: 3053.2834\n",
            "Epoch 62/300\n",
            " - 0s - loss: 3086.0414 - combined_loss: 121.5112 - granger_loss: 6.1711 - repeat_loss: 2967.9903 - val_loss: 3097.5401 - val_combined_loss: 130.3590 - val_granger_loss: 6.2382 - val_repeat_loss: 2970.9048\n",
            "Epoch 63/300\n",
            " - 0s - loss: 3020.5950 - combined_loss: 118.2662 - granger_loss: 6.1543 - repeat_loss: 2905.6921 - val_loss: 3013.6826 - val_combined_loss: 126.3388 - val_granger_loss: 6.2231 - val_repeat_loss: 2890.9473\n",
            "Epoch 64/300\n",
            " - 0s - loss: 2903.2492 - combined_loss: 113.3010 - granger_loss: 6.1392 - repeat_loss: 2793.1631 - val_loss: 2932.1689 - val_combined_loss: 122.6543 - val_granger_loss: 6.1964 - val_repeat_loss: 2813.0083\n",
            "Epoch 65/300\n",
            " - 0s - loss: 2813.7182 - combined_loss: 109.0575 - granger_loss: 6.1569 - repeat_loss: 2707.7476 - val_loss: 2850.9727 - val_combined_loss: 119.1035 - val_granger_loss: 6.1723 - val_repeat_loss: 2735.2572\n",
            "Epoch 66/300\n",
            " - 0s - loss: 2763.2339 - combined_loss: 108.1156 - granger_loss: 6.1449 - repeat_loss: 2658.1775 - val_loss: 2772.8818 - val_combined_loss: 115.7734 - val_granger_loss: 6.1309 - val_repeat_loss: 2660.3977\n",
            "Epoch 67/300\n",
            " - 0s - loss: 2661.7506 - combined_loss: 104.0525 - granger_loss: 6.0902 - repeat_loss: 2560.6369 - val_loss: 2696.3812 - val_combined_loss: 112.5600 - val_granger_loss: 6.1138 - val_repeat_loss: 2587.0146\n",
            "Epoch 68/300\n",
            " - 0s - loss: 2645.4525 - combined_loss: 104.3645 - granger_loss: 6.0421 - repeat_loss: 2544.0377 - val_loss: 2621.5648 - val_combined_loss: 109.4049 - val_granger_loss: 5.9594 - val_repeat_loss: 2515.2633\n",
            "Epoch 69/300\n",
            " - 0s - loss: 2547.9358 - combined_loss: 99.9717 - granger_loss: 5.8764 - repeat_loss: 2450.7870 - val_loss: 2549.5806 - val_combined_loss: 106.2942 - val_granger_loss: 5.6840 - val_repeat_loss: 2446.3047\n",
            "Epoch 70/300\n",
            " - 0s - loss: 2451.4663 - combined_loss: 95.7760 - granger_loss: 5.6011 - repeat_loss: 2358.3955 - val_loss: 2479.6314 - val_combined_loss: 103.0267 - val_granger_loss: 5.5132 - val_repeat_loss: 2379.5301\n",
            "Epoch 71/300\n",
            " - 0s - loss: 2418.8750 - combined_loss: 95.0795 - granger_loss: 5.3256 - repeat_loss: 2326.4881 - val_loss: 2413.4978 - val_combined_loss: 99.9270 - val_granger_loss: 5.0791 - val_repeat_loss: 2316.4163\n",
            "Epoch 72/300\n",
            " - 0s - loss: 2346.9790 - combined_loss: 92.0214 - granger_loss: 4.7555 - repeat_loss: 2257.5756 - val_loss: 2348.9412 - val_combined_loss: 97.3275 - val_granger_loss: 4.7853 - val_repeat_loss: 2254.3899\n",
            "Epoch 73/300\n",
            " - 0s - loss: 2288.1343 - combined_loss: 90.2420 - granger_loss: 4.7175 - repeat_loss: 2200.4580 - val_loss: 2286.1620 - val_combined_loss: 95.0063 - val_granger_loss: 4.5076 - val_repeat_loss: 2193.8706\n",
            "Epoch 74/300\n",
            " - 0s - loss: 2253.3428 - combined_loss: 89.4055 - granger_loss: 4.2545 - repeat_loss: 2166.4918 - val_loss: 2224.5392 - val_combined_loss: 92.0561 - val_granger_loss: 3.9839 - val_repeat_loss: 2135.1254\n",
            "Epoch 75/300\n",
            " - 0s - loss: 2193.5149 - combined_loss: 85.9457 - granger_loss: 3.5657 - repeat_loss: 2110.0405 - val_loss: 2161.8702 - val_combined_loss: 86.0827 - val_granger_loss: 3.0514 - val_repeat_loss: 2078.2784\n",
            "Epoch 76/300\n",
            " - 0s - loss: 2123.8330 - combined_loss: 75.6932 - granger_loss: 2.6137 - repeat_loss: 2050.3322 - val_loss: 2092.9840 - val_combined_loss: 72.6487 - val_granger_loss: 2.4517 - val_repeat_loss: 2022.4411\n",
            "Epoch 77/300\n",
            " - 0s - loss: 2044.6860 - combined_loss: 62.6011 - granger_loss: 2.2266 - repeat_loss: 1983.8962 - val_loss: 2029.7358 - val_combined_loss: 61.6664 - val_granger_loss: 2.1751 - val_repeat_loss: 1969.8541\n",
            "Epoch 78/300\n",
            " - 0s - loss: 2002.0559 - combined_loss: 57.1333 - granger_loss: 2.1508 - repeat_loss: 1946.5720 - val_loss: 1972.7143 - val_combined_loss: 55.2072 - val_granger_loss: 2.3205 - val_repeat_loss: 1919.0936\n",
            "Epoch 79/300\n",
            " - 0s - loss: 1954.0279 - combined_loss: 52.3078 - granger_loss: 2.2879 - repeat_loss: 1903.2206 - val_loss: 1920.4565 - val_combined_loss: 50.8935 - val_granger_loss: 2.4893 - val_repeat_loss: 1871.0151\n",
            "Epoch 80/300\n",
            " - 0s - loss: 1924.4869 - combined_loss: 48.8440 - granger_loss: 2.4026 - repeat_loss: 1877.0361 - val_loss: 1873.3574 - val_combined_loss: 48.5376 - val_granger_loss: 2.5087 - val_repeat_loss: 1826.2006\n",
            "Epoch 81/300\n",
            " - 0s - loss: 1830.9036 - combined_loss: 44.5191 - granger_loss: 2.4295 - repeat_loss: 1787.6471 - val_loss: 1825.4367 - val_combined_loss: 44.8933 - val_granger_loss: 2.4872 - val_repeat_loss: 1781.8155\n",
            "Epoch 82/300\n",
            " - 0s - loss: 1841.6473 - combined_loss: 44.9543 - granger_loss: 2.3550 - repeat_loss: 1797.9709 - val_loss: 1781.8114 - val_combined_loss: 42.5419 - val_granger_loss: 2.3914 - val_repeat_loss: 1740.4740\n",
            "Epoch 83/300\n",
            " - 0s - loss: 1803.0612 - combined_loss: 43.1584 - granger_loss: 2.2838 - repeat_loss: 1761.1291 - val_loss: 1741.1228 - val_combined_loss: 40.5672 - val_granger_loss: 2.3785 - val_repeat_loss: 1701.7012\n",
            "Epoch 84/300\n",
            " - 0s - loss: 1761.0088 - combined_loss: 40.2304 - granger_loss: 2.2788 - repeat_loss: 1721.9170 - val_loss: 1703.2678 - val_combined_loss: 38.5403 - val_granger_loss: 2.3199 - val_repeat_loss: 1665.8140\n",
            "Epoch 85/300\n",
            " - 0s - loss: 1693.5673 - combined_loss: 37.8443 - granger_loss: 2.2360 - repeat_loss: 1656.7912 - val_loss: 1664.8417 - val_combined_loss: 36.4326 - val_granger_loss: 2.2409 - val_repeat_loss: 1629.4348\n",
            "Epoch 86/300\n",
            " - 0s - loss: 1642.4073 - combined_loss: 35.0389 - granger_loss: 2.1957 - repeat_loss: 1608.3537 - val_loss: 1631.3448 - val_combined_loss: 34.7551 - val_granger_loss: 2.1908 - val_repeat_loss: 1597.5666\n",
            "Epoch 87/300\n",
            " - 0s - loss: 1610.6459 - combined_loss: 33.4326 - granger_loss: 2.1020 - repeat_loss: 1578.1532 - val_loss: 1599.6456 - val_combined_loss: 33.4178 - val_granger_loss: 2.0821 - val_repeat_loss: 1567.1679\n",
            "Epoch 88/300\n",
            " - 0s - loss: 1601.0488 - combined_loss: 31.9303 - granger_loss: 2.0324 - repeat_loss: 1570.0154 - val_loss: 1569.6685 - val_combined_loss: 32.0427 - val_granger_loss: 2.0352 - val_repeat_loss: 1538.5260\n",
            "Epoch 89/300\n",
            " - 0s - loss: 1557.8066 - combined_loss: 30.0087 - granger_loss: 2.0161 - repeat_loss: 1528.6377 - val_loss: 1539.9508 - val_combined_loss: 30.7732 - val_granger_loss: 2.0171 - val_repeat_loss: 1510.0403\n",
            "Epoch 90/300\n",
            " - 0s - loss: 1548.8373 - combined_loss: 29.1971 - granger_loss: 1.9916 - repeat_loss: 1520.4564 - val_loss: 1512.9442 - val_combined_loss: 29.5194 - val_granger_loss: 2.0086 - val_repeat_loss: 1484.2501\n",
            "Epoch 91/300\n",
            " - 0s - loss: 1511.0936 - combined_loss: 27.3533 - granger_loss: 1.9367 - repeat_loss: 1484.5028 - val_loss: 1487.3317 - val_combined_loss: 28.5253 - val_granger_loss: 1.9534 - val_repeat_loss: 1459.6035\n",
            "Epoch 92/300\n",
            " - 0s - loss: 1494.4838 - combined_loss: 27.0052 - granger_loss: 1.9563 - repeat_loss: 1468.2301 - val_loss: 1461.7855 - val_combined_loss: 27.2232 - val_granger_loss: 1.9355 - val_repeat_loss: 1435.3210\n",
            "Epoch 93/300\n",
            " - 0s - loss: 1461.4795 - combined_loss: 25.0221 - granger_loss: 1.9462 - repeat_loss: 1437.1497 - val_loss: 1440.4499 - val_combined_loss: 26.2177 - val_granger_loss: 1.9134 - val_repeat_loss: 1414.9614\n",
            "Epoch 94/300\n",
            " - 0s - loss: 1437.0629 - combined_loss: 24.1647 - granger_loss: 1.9167 - repeat_loss: 1413.5657 - val_loss: 1420.9942 - val_combined_loss: 25.3461 - val_granger_loss: 1.9098 - val_repeat_loss: 1396.3512\n",
            "Epoch 95/300\n",
            " - 0s - loss: 1423.1825 - combined_loss: 23.0347 - granger_loss: 1.9011 - repeat_loss: 1400.7818 - val_loss: 1400.2932 - val_combined_loss: 24.3139 - val_granger_loss: 1.9211 - val_repeat_loss: 1376.6511\n",
            "Epoch 96/300\n",
            " - 0s - loss: 1431.3052 - combined_loss: 23.6162 - granger_loss: 1.8911 - repeat_loss: 1408.3408 - val_loss: 1382.5880 - val_combined_loss: 23.4864 - val_granger_loss: 1.8548 - val_repeat_loss: 1359.7506\n",
            "Epoch 97/300\n",
            " - 0s - loss: 1370.3297 - combined_loss: 21.4462 - granger_loss: 1.8838 - repeat_loss: 1349.4703 - val_loss: 1365.6190 - val_combined_loss: 22.8523 - val_granger_loss: 1.8399 - val_repeat_loss: 1343.3970\n",
            "Epoch 98/300\n",
            " - 0s - loss: 1353.3832 - combined_loss: 20.9275 - granger_loss: 1.8743 - repeat_loss: 1333.0272 - val_loss: 1350.0507 - val_combined_loss: 22.1035 - val_granger_loss: 1.8740 - val_repeat_loss: 1328.5540\n",
            "Epoch 99/300\n",
            " - 0s - loss: 1381.2290 - combined_loss: 21.7149 - granger_loss: 1.8738 - repeat_loss: 1360.1093 - val_loss: 1335.5446 - val_combined_loss: 21.4752 - val_granger_loss: 1.8607 - val_repeat_loss: 1314.6578\n",
            "Epoch 100/300\n",
            " - 0s - loss: 1341.1944 - combined_loss: 20.0415 - granger_loss: 1.9125 - repeat_loss: 1321.6968 - val_loss: 1322.5338 - val_combined_loss: 20.9918 - val_granger_loss: 1.9154 - val_repeat_loss: 1302.1143\n",
            "Epoch 101/300\n",
            " - 0s - loss: 1325.0199 - combined_loss: 19.3559 - granger_loss: 1.8678 - repeat_loss: 1306.1887 - val_loss: 1310.4676 - val_combined_loss: 20.3811 - val_granger_loss: 1.8813 - val_repeat_loss: 1290.6414\n",
            "Epoch 102/300\n",
            " - 0s - loss: 1325.2406 - combined_loss: 19.7725 - granger_loss: 1.8583 - repeat_loss: 1306.0056 - val_loss: 1298.7950 - val_combined_loss: 19.8583 - val_granger_loss: 1.8735 - val_repeat_loss: 1279.4763\n",
            "Epoch 103/300\n",
            " - 0s - loss: 1293.6235 - combined_loss: 18.9193 - granger_loss: 1.9111 - repeat_loss: 1275.2145 - val_loss: 1289.7013 - val_combined_loss: 19.6199 - val_granger_loss: 1.8880 - val_repeat_loss: 1270.6134\n",
            "Epoch 104/300\n",
            " - 0s - loss: 1282.7804 - combined_loss: 18.4683 - granger_loss: 1.8960 - repeat_loss: 1264.8093 - val_loss: 1280.2609 - val_combined_loss: 19.0476 - val_granger_loss: 1.9017 - val_repeat_loss: 1261.7277\n",
            "Epoch 105/300\n",
            " - 0s - loss: 1275.9932 - combined_loss: 18.2115 - granger_loss: 1.8784 - repeat_loss: 1258.2718 - val_loss: 1270.5641 - val_combined_loss: 18.8284 - val_granger_loss: 1.8742 - val_repeat_loss: 1252.2443\n",
            "Epoch 106/300\n",
            " - 0s - loss: 1312.6406 - combined_loss: 18.9923 - granger_loss: 1.8867 - repeat_loss: 1294.1614 - val_loss: 1262.0673 - val_combined_loss: 18.3066 - val_granger_loss: 1.9277 - val_repeat_loss: 1244.2521\n",
            "Epoch 107/300\n",
            " - 0s - loss: 1264.2351 - combined_loss: 17.7305 - granger_loss: 1.9067 - repeat_loss: 1246.9793 - val_loss: 1255.0674 - val_combined_loss: 18.2500 - val_granger_loss: 1.8442 - val_repeat_loss: 1237.3095\n",
            "Epoch 108/300\n",
            " - 0s - loss: 1251.6380 - combined_loss: 17.5069 - granger_loss: 1.8694 - repeat_loss: 1234.6001 - val_loss: 1248.3391 - val_combined_loss: 17.8140 - val_granger_loss: 1.9428 - val_repeat_loss: 1231.0012\n",
            "Epoch 109/300\n",
            " - 0s - loss: 1256.7615 - combined_loss: 17.4703 - granger_loss: 1.9450 - repeat_loss: 1239.7570 - val_loss: 1242.0131 - val_combined_loss: 17.6906 - val_granger_loss: 1.8831 - val_repeat_loss: 1224.7967\n",
            "Epoch 110/300\n",
            " - 0s - loss: 1259.5697 - combined_loss: 17.4138 - granger_loss: 1.8791 - repeat_loss: 1242.6220 - val_loss: 1235.7392 - val_combined_loss: 17.4429 - val_granger_loss: 1.8836 - val_repeat_loss: 1218.7630\n",
            "Epoch 111/300\n",
            " - 0s - loss: 1254.6876 - combined_loss: 17.0102 - granger_loss: 1.9304 - repeat_loss: 1238.1297 - val_loss: 1230.6619 - val_combined_loss: 17.0006 - val_granger_loss: 1.9826 - val_repeat_loss: 1214.1119\n",
            "Epoch 112/300\n",
            " - 0s - loss: 1261.0010 - combined_loss: 17.8448 - granger_loss: 1.9113 - repeat_loss: 1243.6342 - val_loss: 1224.4346 - val_combined_loss: 16.9672 - val_granger_loss: 1.9333 - val_repeat_loss: 1207.9185\n",
            "Epoch 113/300\n",
            " - 0s - loss: 1264.8919 - combined_loss: 17.2326 - granger_loss: 1.9171 - repeat_loss: 1248.1188 - val_loss: 1220.7373 - val_combined_loss: 16.7650 - val_granger_loss: 1.9528 - val_repeat_loss: 1204.4167\n",
            "Epoch 114/300\n",
            " - 0s - loss: 1220.0323 - combined_loss: 16.5038 - granger_loss: 1.9014 - repeat_loss: 1203.9665 - val_loss: 1215.6798 - val_combined_loss: 16.5720 - val_granger_loss: 1.9485 - val_repeat_loss: 1199.5465\n",
            "Epoch 115/300\n",
            " - 0s - loss: 1228.9766 - combined_loss: 16.3402 - granger_loss: 1.9602 - repeat_loss: 1213.0678 - val_loss: 1212.7103 - val_combined_loss: 16.6176 - val_granger_loss: 1.9505 - val_repeat_loss: 1196.5328\n",
            "Epoch 116/300\n",
            " - 0s - loss: 1252.6674 - combined_loss: 17.1345 - granger_loss: 1.8705 - repeat_loss: 1235.9909 - val_loss: 1207.5800 - val_combined_loss: 16.2858 - val_granger_loss: 1.9885 - val_repeat_loss: 1191.7232\n",
            "Epoch 117/300\n",
            " - 0s - loss: 1221.4483 - combined_loss: 16.3810 - granger_loss: 1.9349 - repeat_loss: 1205.5007 - val_loss: 1204.4373 - val_combined_loss: 16.2857 - val_granger_loss: 1.9674 - val_repeat_loss: 1188.5811\n",
            "Epoch 118/300\n",
            " - 0s - loss: 1235.2611 - combined_loss: 16.3961 - granger_loss: 1.9415 - repeat_loss: 1219.2986 - val_loss: 1202.6747 - val_combined_loss: 16.1342 - val_granger_loss: 2.0459 - val_repeat_loss: 1186.9632\n",
            "Epoch 119/300\n",
            " - 0s - loss: 1210.0691 - combined_loss: 16.0681 - granger_loss: 1.9479 - repeat_loss: 1194.4247 - val_loss: 1199.5840 - val_combined_loss: 16.2603 - val_granger_loss: 1.9403 - val_repeat_loss: 1183.7533\n",
            "Epoch 120/300\n",
            " - 0s - loss: 1230.5114 - combined_loss: 15.9598 - granger_loss: 1.9761 - repeat_loss: 1214.9710 - val_loss: 1198.2321 - val_combined_loss: 16.0323 - val_granger_loss: 2.0648 - val_repeat_loss: 1182.6188\n",
            "Epoch 121/300\n",
            " - 0s - loss: 1196.7076 - combined_loss: 15.6590 - granger_loss: 1.9009 - repeat_loss: 1181.4613 - val_loss: 1194.2773 - val_combined_loss: 16.1904 - val_granger_loss: 2.0000 - val_repeat_loss: 1178.5126\n",
            "Epoch 122/300\n",
            " - 0s - loss: 1209.0314 - combined_loss: 15.9013 - granger_loss: 1.9655 - repeat_loss: 1193.5482 - val_loss: 1190.6116 - val_combined_loss: 15.9885 - val_granger_loss: 2.0148 - val_repeat_loss: 1175.0423\n",
            "Epoch 123/300\n",
            " - 0s - loss: 1209.5857 - combined_loss: 15.4847 - granger_loss: 1.9568 - repeat_loss: 1194.5068 - val_loss: 1190.8183 - val_combined_loss: 16.1314 - val_granger_loss: 2.0186 - val_repeat_loss: 1175.1104\n",
            "Epoch 124/300\n",
            " - 0s - loss: 1201.8290 - combined_loss: 15.3284 - granger_loss: 1.9821 - repeat_loss: 1186.9011 - val_loss: 1188.6487 - val_combined_loss: 16.0039 - val_granger_loss: 2.0490 - val_repeat_loss: 1173.0634\n",
            "Epoch 125/300\n",
            " - 0s - loss: 1229.5275 - combined_loss: 16.1336 - granger_loss: 1.9603 - repeat_loss: 1213.8191 - val_loss: 1186.9123 - val_combined_loss: 16.0195 - val_granger_loss: 2.0487 - val_repeat_loss: 1171.3119\n",
            "Epoch 126/300\n",
            " - 0s - loss: 1191.0664 - combined_loss: 14.9914 - granger_loss: 1.9636 - repeat_loss: 1176.4658 - val_loss: 1186.5002 - val_combined_loss: 16.0449 - val_granger_loss: 2.0914 - val_repeat_loss: 1170.8739\n",
            "Epoch 127/300\n",
            " - 0s - loss: 1188.6106 - combined_loss: 15.0386 - granger_loss: 2.0051 - repeat_loss: 1173.9630 - val_loss: 1184.6759 - val_combined_loss: 16.0915 - val_granger_loss: 2.1261 - val_repeat_loss: 1169.0034\n",
            "Epoch 128/300\n",
            " - 0s - loss: 1195.1582 - combined_loss: 15.0698 - granger_loss: 2.0245 - repeat_loss: 1180.4798 - val_loss: 1183.7575 - val_combined_loss: 16.0066 - val_granger_loss: 2.1066 - val_repeat_loss: 1168.1680\n",
            "Epoch 129/300\n",
            " - 0s - loss: 1211.5397 - combined_loss: 15.0219 - granger_loss: 2.0191 - repeat_loss: 1196.9079 - val_loss: 1182.5427 - val_combined_loss: 16.1715 - val_granger_loss: 2.0215 - val_repeat_loss: 1166.7957\n",
            "Epoch 130/300\n",
            " - 0s - loss: 1193.4032 - combined_loss: 15.2559 - granger_loss: 1.9778 - repeat_loss: 1178.5457 - val_loss: 1181.7229 - val_combined_loss: 15.9276 - val_granger_loss: 2.1323 - val_repeat_loss: 1166.2091\n",
            "Epoch 131/300\n",
            " - 0s - loss: 1175.2917 - combined_loss: 14.9023 - granger_loss: 2.0094 - repeat_loss: 1160.7762 - val_loss: 1180.0610 - val_combined_loss: 16.0002 - val_granger_loss: 2.1154 - val_repeat_loss: 1164.4774\n",
            "Epoch 132/300\n",
            " - 0s - loss: 1183.2219 - combined_loss: 14.9645 - granger_loss: 2.0344 - repeat_loss: 1168.6452 - val_loss: 1180.6714 - val_combined_loss: 16.2939 - val_granger_loss: 2.0751 - val_repeat_loss: 1164.8040\n",
            "Epoch 133/300\n",
            " - 0s - loss: 1173.2132 - combined_loss: 14.7975 - granger_loss: 1.9962 - repeat_loss: 1158.7998 - val_loss: 1178.5640 - val_combined_loss: 15.9084 - val_granger_loss: 2.1679 - val_repeat_loss: 1163.0678\n",
            "Epoch 134/300\n",
            " - 0s - loss: 1171.2582 - combined_loss: 14.7379 - granger_loss: 2.0954 - repeat_loss: 1156.8995 - val_loss: 1178.1484 - val_combined_loss: 16.2694 - val_granger_loss: 2.0953 - val_repeat_loss: 1162.3042\n",
            "Epoch 135/300\n",
            " - 0s - loss: 1206.5707 - combined_loss: 15.2097 - granger_loss: 2.0112 - repeat_loss: 1191.7569 - val_loss: 1178.8504 - val_combined_loss: 16.0176 - val_granger_loss: 2.2067 - val_repeat_loss: 1163.2472\n",
            "Epoch 136/300\n",
            " - 0s - loss: 1197.1719 - combined_loss: 15.1344 - granger_loss: 2.0354 - repeat_loss: 1182.4304 - val_loss: 1177.7415 - val_combined_loss: 16.2427 - val_granger_loss: 2.1134 - val_repeat_loss: 1161.9226\n",
            "Epoch 137/300\n",
            " - 0s - loss: 1168.8046 - combined_loss: 14.8938 - granger_loss: 2.0371 - repeat_loss: 1154.2965 - val_loss: 1176.1721 - val_combined_loss: 15.9874 - val_granger_loss: 2.1664 - val_repeat_loss: 1160.5993\n",
            "Epoch 138/300\n",
            " - 0s - loss: 1182.0945 - combined_loss: 15.0149 - granger_loss: 2.0696 - repeat_loss: 1167.4679 - val_loss: 1176.2810 - val_combined_loss: 16.1589 - val_granger_loss: 2.1398 - val_repeat_loss: 1160.5427\n",
            "Epoch 139/300\n",
            " - 0s - loss: 1173.6557 - combined_loss: 14.6044 - granger_loss: 2.0461 - repeat_loss: 1159.4281 - val_loss: 1175.2901 - val_combined_loss: 15.9877 - val_granger_loss: 2.2242 - val_repeat_loss: 1159.7153\n",
            "Epoch 140/300\n",
            " - 0s - loss: 1191.4236 - combined_loss: 15.0639 - granger_loss: 2.0698 - repeat_loss: 1176.7495 - val_loss: 1174.4768 - val_combined_loss: 16.0184 - val_granger_loss: 2.1975 - val_repeat_loss: 1158.8731\n",
            "Epoch 141/300\n",
            " - 0s - loss: 1185.4383 - combined_loss: 14.7334 - granger_loss: 2.0284 - repeat_loss: 1171.0861 - val_loss: 1175.7315 - val_combined_loss: 16.0931 - val_granger_loss: 2.1875 - val_repeat_loss: 1160.0556\n",
            "Epoch 142/300\n",
            " - 0s - loss: 1213.2634 - combined_loss: 15.1072 - granger_loss: 2.0595 - repeat_loss: 1198.5476 - val_loss: 1175.1269 - val_combined_loss: 16.0243 - val_granger_loss: 2.2113 - val_repeat_loss: 1159.5170\n",
            "Epoch 143/300\n",
            " - 0s - loss: 1204.2588 - combined_loss: 14.8102 - granger_loss: 2.0900 - repeat_loss: 1189.8302 - val_loss: 1175.2715 - val_combined_loss: 16.0106 - val_granger_loss: 2.2000 - val_repeat_loss: 1159.6753\n",
            "\n",
            "Epoch 00143: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.\n",
            "Epoch 144/300\n",
            " - 0s - loss: 1170.6955 - combined_loss: 14.5269 - granger_loss: 2.0701 - repeat_loss: 1156.5423 - val_loss: 1174.9051 - val_combined_loss: 16.0681 - val_granger_loss: 2.2148 - val_repeat_loss: 1159.2526\n",
            "Epoch 145/300\n",
            " - 0s - loss: 1204.1881 - combined_loss: 15.4625 - granger_loss: 2.0960 - repeat_loss: 1189.1266 - val_loss: 1174.7784 - val_combined_loss: 15.9830 - val_granger_loss: 2.2502 - val_repeat_loss: 1159.2073\n",
            "Epoch 146/300\n",
            " - 0s - loss: 1195.2504 - combined_loss: 14.8176 - granger_loss: 2.0698 - repeat_loss: 1180.8153 - val_loss: 1175.3605 - val_combined_loss: 16.0816 - val_granger_loss: 2.1876 - val_repeat_loss: 1159.6957\n",
            "Epoch 147/300\n",
            " - 0s - loss: 1172.2227 - combined_loss: 14.3487 - granger_loss: 2.0469 - repeat_loss: 1158.2430 - val_loss: 1174.8421 - val_combined_loss: 16.0505 - val_granger_loss: 2.2151 - val_repeat_loss: 1159.2067\n",
            "Epoch 148/300\n",
            " - 0s - loss: 1169.7210 - combined_loss: 14.5237 - granger_loss: 2.0680 - repeat_loss: 1155.5710 - val_loss: 1175.0122 - val_combined_loss: 16.0597 - val_granger_loss: 2.2219 - val_repeat_loss: 1159.3677\n",
            "Epoch 149/300\n",
            " - 0s - loss: 1166.2839 - combined_loss: 14.4860 - granger_loss: 2.0548 - repeat_loss: 1152.1708 - val_loss: 1175.0827 - val_combined_loss: 16.1751 - val_granger_loss: 2.2024 - val_repeat_loss: 1159.3267\n",
            "Epoch 150/300\n",
            " - 0s - loss: 1163.2448 - combined_loss: 14.3798 - granger_loss: 2.0619 - repeat_loss: 1149.2345 - val_loss: 1175.0734 - val_combined_loss: 16.0231 - val_granger_loss: 2.2527 - val_repeat_loss: 1159.4634\n",
            "Epoch 151/300\n",
            " - 0s - loss: 1175.1888 - combined_loss: 14.4147 - granger_loss: 2.0910 - repeat_loss: 1161.1439 - val_loss: 1174.8777 - val_combined_loss: 16.0691 - val_granger_loss: 2.2141 - val_repeat_loss: 1159.2242\n",
            "Epoch 152/300\n",
            " - 0s - loss: 1196.6268 - combined_loss: 15.1502 - granger_loss: 2.0609 - repeat_loss: 1181.8693 - val_loss: 1174.8473 - val_combined_loss: 16.0624 - val_granger_loss: 2.2231 - val_repeat_loss: 1159.2000\n",
            "\n",
            "Epoch 00152: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.\n",
            "Epoch 153/300\n",
            " - 0s - loss: 1165.7379 - combined_loss: 14.3130 - granger_loss: 2.0796 - repeat_loss: 1151.7919 - val_loss: 1174.8758 - val_combined_loss: 16.0748 - val_granger_loss: 2.2278 - val_repeat_loss: 1159.2165\n",
            "Epoch 154/300\n",
            " - 0s - loss: 1159.0417 - combined_loss: 14.2973 - granger_loss: 2.0788 - repeat_loss: 1145.1110 - val_loss: 1174.7767 - val_combined_loss: 16.0629 - val_granger_loss: 2.2332 - val_repeat_loss: 1159.1287\n",
            "Epoch 155/300\n",
            " - 0s - loss: 1166.5106 - combined_loss: 14.4065 - granger_loss: 2.0791 - repeat_loss: 1152.4739 - val_loss: 1174.8779 - val_combined_loss: 16.0977 - val_granger_loss: 2.2184 - val_repeat_loss: 1159.1966\n",
            "Epoch 156/300\n",
            " - 0s - loss: 1188.0293 - combined_loss: 15.1097 - granger_loss: 2.0612 - repeat_loss: 1173.3111 - val_loss: 1174.9402 - val_combined_loss: 16.0966 - val_granger_loss: 2.2199 - val_repeat_loss: 1159.2599\n",
            "Epoch 157/300\n",
            " - 0s - loss: 1168.0409 - combined_loss: 14.3479 - granger_loss: 2.0751 - repeat_loss: 1154.0612 - val_loss: 1174.8778 - val_combined_loss: 16.0765 - val_granger_loss: 2.2334 - val_repeat_loss: 1159.2166\n",
            "Epoch 158/300\n",
            " - 0s - loss: 1166.5576 - combined_loss: 14.2265 - granger_loss: 2.0704 - repeat_loss: 1152.6958 - val_loss: 1174.7965 - val_combined_loss: 16.0948 - val_granger_loss: 2.2288 - val_repeat_loss: 1159.1177\n",
            "Epoch 159/300\n",
            " - 0s - loss: 1158.6159 - combined_loss: 14.4071 - granger_loss: 2.0572 - repeat_loss: 1144.5793 - val_loss: 1175.0120 - val_combined_loss: 16.1119 - val_granger_loss: 2.2257 - val_repeat_loss: 1159.3167\n",
            "Epoch 160/300\n",
            " - 0s - loss: 1170.8615 - combined_loss: 14.3261 - granger_loss: 2.0691 - repeat_loss: 1156.9031 - val_loss: 1174.9331 - val_combined_loss: 16.0645 - val_granger_loss: 2.2362 - val_repeat_loss: 1159.2835\n",
            "Epoch 161/300\n",
            " - 0s - loss: 1190.1712 - combined_loss: 14.2957 - granger_loss: 2.0809 - repeat_loss: 1176.2419 - val_loss: 1174.9366 - val_combined_loss: 16.0598 - val_granger_loss: 2.2412 - val_repeat_loss: 1159.2913\n",
            "\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 3.1622778103685084e-05.\n",
            "Epoch 162/300\n",
            " - 0s - loss: 1157.8858 - combined_loss: 14.2546 - granger_loss: 2.0856 - repeat_loss: 1143.9962 - val_loss: 1174.8896 - val_combined_loss: 16.0646 - val_granger_loss: 2.2385 - val_repeat_loss: 1159.2398\n",
            "Epoch 163/300\n",
            " - 0s - loss: 1160.3374 - combined_loss: 14.2725 - granger_loss: 2.0670 - repeat_loss: 1146.4311 - val_loss: 1174.8976 - val_combined_loss: 16.0666 - val_granger_loss: 2.2365 - val_repeat_loss: 1159.2460\n",
            "Epoch 164/300\n",
            " - 0s - loss: 1163.4797 - combined_loss: 14.3749 - granger_loss: 2.0820 - repeat_loss: 1149.4736 - val_loss: 1174.9690 - val_combined_loss: 16.0783 - val_granger_loss: 2.2326 - val_repeat_loss: 1159.3060\n",
            "Epoch 165/300\n",
            " - 0s - loss: 1173.0239 - combined_loss: 14.3362 - granger_loss: 2.0612 - repeat_loss: 1159.0559 - val_loss: 1174.9025 - val_combined_loss: 16.0767 - val_granger_loss: 2.2332 - val_repeat_loss: 1159.2411\n",
            "INFO: Saving loss history to /content/drive/My Drive/pr2/ame_output/losses.pickle\n",
            "INFO: Started evaluation.\n",
            "Eval score: {'loss': 1174.9025065104167, 'combined_loss': 16.076666588876762, 'granger_loss': 2.2331967026579616, 'repeat_loss': 1159.2411343443628}\n",
            "INFO: Saving model predictions.\n",
            "INFO: Loaded generator with 404 samples. Doing 13 steps of size 32\n",
            "INFO: Saved model predictions to /content/drive/My Drive/pr2/ame_output/train_predictions.csv\n",
            "INFO: Loaded generator with 102 samples. Doing 4 steps of size 32\n",
            "INFO: Saved model predictions to /content/drive/My Drive/pr2/ame_output/val_predictions.csv\n",
            "INFO: Loaded generator with 102 samples. Doing 4 steps of size 32\n",
            "INFO: Saved model predictions to /content/drive/My Drive/pr2/ame_output/test_predictions.csv\n",
            "INFO: Saving model attributions.\n",
            "INFO: Loaded generator with 404 samples. Doing 13 steps of size 32\n",
            "INFO: Saved model predictions to /content/drive/My Drive/pr2/ame_output/train_attributions.csv\n",
            "INFO: Loaded generator with 102 samples. Doing 4 steps of size 32\n",
            "INFO: Saved model predictions to /content/drive/My Drive/pr2/ame_output/val_attributions.csv\n",
            "INFO: Loaded generator with 102 samples. Doing 4 steps of size 32\n",
            "INFO: Saved model predictions to /content/drive/My Drive/pr2/ame_output/test_attributions.csv\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ame_setup.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}